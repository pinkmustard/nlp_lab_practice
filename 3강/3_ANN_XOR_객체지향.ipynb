{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 객체지향 프로그램으로 하면 좋은 점\n",
        "config에 각 값 세팅해서 key값으로 불러오면 편함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AFmjvMrPqgOY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n",
        "\n",
        "# 각 노드를 몇개를 설정하고, 연결하고 layer를 몇층을 쌓을 것인지\n",
        "class XOR(nn.Module): # 무조건 nn.Module를 상속 받아야함.\n",
        "    def __init__(self, config): # 생성자 오버라이드 해야함.\n",
        "        super(XOR, self).__init__()\n",
        "\n",
        "        # 입력층 노드 수 : 2\n",
        "        self.inode = config['input_node']\n",
        "        # 은닉층 데이터 크기 : 10\n",
        "        self.hnode = config['hidden_node']\n",
        "        # 출력층 노드 수 : 1, 분류해야하는 레이블 수\n",
        "        self.onode = config['output_node']\n",
        "\n",
        "        # 활성화 함수로 Sigmoid 사용\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "        # 신경망 설계\n",
        "        self.linear1 = nn.Linear(self.inode, self.hnode, bias=True) # 2*10\n",
        "        self.linear2 = nn.Linear(self.hnode, self.onode, bias=True) # 10*1\n",
        "\n",
        "    def forward(self, input_features): # 포워드도 오버라이드 해야함. # foward는 Hypothesis만드는 작업.\n",
        "\n",
        "        output1 = self.linear1(input_features) # 인풋을 넣어서 아웃풋 하나 뽑아냄.\n",
        "        hypothesis1 = self.activation(output1) # 시그모이드\n",
        "\n",
        "        output2 = self.linear2(hypothesis1) # 그걸 다음 층에 넣음\n",
        "        hypothesis2 = self.activation(output2) # 시그모이드\n",
        "\n",
        "        return hypothesis2 # 최종 hypothesis 리턴"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sPJqgJZyqgOb"
      },
      "outputs": [],
      "source": [
        "# 데이터 읽기 함수\n",
        "def load_dataset(file, device):\n",
        "    data = np.loadtxt(file)\n",
        "    print(\"DATA=\", data)\n",
        "\n",
        "    input_features = data[:, 0:-1]\n",
        "    print(\"INPUT_FEATURES=\", input_features)\n",
        "\n",
        "    labels = np.reshape(data[:, -1],(4,1))\n",
        "    print(\"LABELS=\", labels)\n",
        "\n",
        "    input_features = torch.tensor(input_features, dtype=torch.float).to(device)\n",
        "    labels = torch.tensor(labels, dtype=torch.float).to(device)\n",
        "\n",
        "    return (input_features, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bcYAV1ihqgOb"
      },
      "outputs": [],
      "source": [
        "# 모델 학습 함수\n",
        "def train(config):\n",
        "\n",
        "    # 모델 생성\n",
        "    model = XOR(config) # .to()하거나 쿠다 지우면 CPU\n",
        "\n",
        "    # 데이터 읽기\n",
        "    (input_features, labels) = load_dataset(config[\"input_data\"], \"cpu\")\n",
        "\n",
        "    # 여기 중요\n",
        "    # tensorDataset/DataLoader를 통해 배치 단위로 데이터를 나누고 / 셔플\n",
        "    train_features = TensorDataset(input_features, labels)\n",
        "    train_dataloader = DataLoader(train_features, shuffle= True, batch_size=config[\"batch_size\"]) # 배치대로 가져오는데 셔플해서 가져옴\n",
        "\n",
        "    # 비용함수 BCE\n",
        "    loss_func = nn.BCELoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=config[\"learn_rate\"])\n",
        "\n",
        "    for epoch in range(config[\"epoch\"]+1): # 에폭만큼 수행\n",
        "\n",
        "        # 학습 모드 셋팅(드랍아웃 같은 것 하려면 train으로. 학습이 아니라 모드 셋팅임)\n",
        "        model.train()\n",
        "\n",
        "        # epoch 마다 평균 비용을 저장하기 위한 리스트\n",
        "        costs = []\n",
        "\n",
        "        for (step, batch) in enumerate(train_dataloader): # 배치만큼 셔플해서 가져온 것\n",
        "            # cpu를 통해 메모리에 업로드\n",
        "            batch = tuple(t.to(\"cpu\") for t in batch) # 배치에 하나씩 가져와서 쿠다에 업로드하는 과정\n",
        "\n",
        "            # 각 feature 저장\n",
        "            input_features, labels = batch\n",
        "            \n",
        "            # 역전파 변화도 초기화\n",
        "            # backward() 호출 시, 변화도 버퍼에 데이터가 계속 누적한 것을 초기화\n",
        "            # 파이토치는 그 전 기울기가 남아있어서 초기화 해야함.\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # H(X) 계산: forward 연산\n",
        "            hypothesis = model(input_features)\n",
        "            # 비용 계산\n",
        "            cost = loss_func(hypothesis, labels)\n",
        "            # 역전파 수행\n",
        "            cost.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # 현재 batch의 스텝 별 loss 저장\n",
        "            costs.append(cost.data.item())\n",
        "            \n",
        "        if epoch%100 == 0:\n",
        "            print(\"Avg Loss={0:f}\".format(np.mean(costs)))\n",
        "            torch.save(model.state_dict(), os.path.join(config[\"output_dir\"], \"epoch_{0:d}.pt\".format(epoch)))\n",
        "            do_test(model, train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZAq3Q5YlqgOb"
      },
      "outputs": [],
      "source": [
        "# 모델 평가 함수\n",
        "def test(config):\n",
        "    model = XOR(config)\n",
        "\n",
        "    # 저장된 모델 가중치 로드\n",
        "    model.load_state_dict(torch.load(os.path.join(config[\"output_dir\"], config[\"model_name\"])))\n",
        "\n",
        "    # 데이터 load\n",
        "    (features, labels) = load_dataset(config[\"input_data\"])\n",
        "\n",
        "    test_features = TensorDataset(features, labels)\n",
        "    test_dataloader = DataLoader(test_features, shuffle=True, batch_size=config[\"batch_size\"])\n",
        "\n",
        "    do_test(model, test_dataloader)\n",
        "\n",
        "def tensor2List(input_tensor):\n",
        "    return input_tensor.cpu().detach().numpy().tolist()\n",
        "\n",
        "# 평가 수행 함수\n",
        "def do_test(model, test_dataloader):\n",
        "\n",
        "    # 평가모드 셋팅\n",
        "    model.eval()\n",
        "\n",
        "    # Batch 별로 예측값과 정답을 저장할 리스트 초기화\n",
        "    predicts, golds = [], []\n",
        "    with torch.no_grad(): # 역전파 하면 안됨.. 평가라서..\n",
        "        for stop, batch in enumerate(test_dataloader):\n",
        "\n",
        "            #.cpu()를 통해 메모리에 업로드\n",
        "            batch = tuple(t.to(\"cpu\") for t in batch)\n",
        "\n",
        "            input_features, labels = batch\n",
        "            hypothesis = model(input_features)\n",
        "            logits = (hypothesis > 0.5).float() # 0, 1로 바꾸는 logit\n",
        "            x = tensor2List(logits)\n",
        "            y = tensor2List(labels)\n",
        "\n",
        "            # 예측값과 정답을 리스트에 추가\n",
        "            predicts.extend(x)\n",
        "            golds.extend(y)\n",
        "\n",
        "        print(\"PRED=\", predicts)\n",
        "        print(\"GOLD=\", golds)\n",
        "        print(\"ACC={0:f}\\n\".format(accuracy_score(golds, predicts)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgb_oDVdqgOc",
        "outputId": "be34e779-01af-4475-a5a6-038aebfb11c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA= [[0. 0. 0.]\n",
            " [0. 1. 1.]\n",
            " [1. 0. 1.]\n",
            " [1. 1. 0.]]\n",
            "INPUT_FEATURES= [[0. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 1.]]\n",
            "LABELS= [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n",
            "Avg Loss=0.700358\n",
            "PRED= [[1.0], [0.0], [1.0], [0.0]]\n",
            "GOLD= [[0.0], [0.0], [1.0], [1.0]]\n",
            "ACC=0.500000\n",
            "\n",
            "Avg Loss=0.692417\n",
            "PRED= [[1.0], [1.0], [0.0], [0.0]]\n",
            "GOLD= [[1.0], [0.0], [1.0], [0.0]]\n",
            "ACC=0.500000\n",
            "\n",
            "Avg Loss=0.689529\n",
            "PRED= [[0.0], [1.0], [1.0], [0.0]]\n",
            "GOLD= [[0.0], [1.0], [0.0], [1.0]]\n",
            "ACC=0.500000\n",
            "\n",
            "Avg Loss=0.664861\n",
            "PRED= [[0.0], [0.0], [0.0], [1.0]]\n",
            "GOLD= [[0.0], [1.0], [0.0], [1.0]]\n",
            "ACC=0.750000\n",
            "\n",
            "Avg Loss=0.417974\n",
            "PRED= [[0.0], [1.0], [0.0], [1.0]]\n",
            "GOLD= [[0.0], [1.0], [0.0], [1.0]]\n",
            "ACC=1.000000\n",
            "\n",
            "Avg Loss=0.113380\n",
            "PRED= [[1.0], [0.0], [0.0], [1.0]]\n",
            "GOLD= [[1.0], [0.0], [0.0], [1.0]]\n",
            "ACC=1.000000\n",
            "\n",
            "Avg Loss=0.049985\n",
            "PRED= [[1.0], [0.0], [0.0], [1.0]]\n",
            "GOLD= [[1.0], [0.0], [0.0], [1.0]]\n",
            "ACC=1.000000\n",
            "\n",
            "Avg Loss=0.030372\n",
            "PRED= [[1.0], [0.0], [1.0], [0.0]]\n",
            "GOLD= [[1.0], [0.0], [1.0], [0.0]]\n",
            "ACC=1.000000\n",
            "\n",
            "Avg Loss=0.021414\n",
            "PRED= [[0.0], [1.0], [0.0], [1.0]]\n",
            "GOLD= [[0.0], [1.0], [0.0], [1.0]]\n",
            "ACC=1.000000\n",
            "\n",
            "Avg Loss=0.016394\n",
            "PRED= [[1.0], [0.0], [1.0], [0.0]]\n",
            "GOLD= [[1.0], [0.0], [1.0], [0.0]]\n",
            "ACC=1.000000\n",
            "\n",
            "Avg Loss=0.013216\n",
            "PRED= [[0.0], [1.0], [1.0], [0.0]]\n",
            "GOLD= [[0.0], [1.0], [1.0], [0.0]]\n",
            "ACC=1.000000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if(__name__==\"__main__\"):\n",
        "    root_dir = '.'\n",
        "    output_dir = os.path.join(root_dir, \"output\")\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    input_data = \"{0:s}/{1:s}\".format(root_dir, 'train.txt')\n",
        "\n",
        "    config = {\"mode\": \"train\", # train, test 모드 선택\n",
        "              \"model_name\": \"epoch_{0:d}.pt\".format(1000), # 테스트때 사용하고 싶은 모델 고를 수 있게. \n",
        "              \"output_dir\": output_dir, # 저장 폴더\n",
        "              \"input_data\": input_data, # 불러올 폴더\n",
        "              \"input_node\": 2,\n",
        "              \"hidden_node\": 10,\n",
        "              \"output_node\": 1,\n",
        "              \"learn_rate\": 1,\n",
        "              \"batch_size\": 4,\n",
        "              \"epoch\": 1000,\n",
        "              }\n",
        "\n",
        "    if(config[\"mode\"] == \"train\"):\n",
        "        train(config)\n",
        "    else:\n",
        "        test(config)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deeplearn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
